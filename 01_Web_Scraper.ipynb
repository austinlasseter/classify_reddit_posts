{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?count=25&after=t3_7ybo2h\n",
      "1\n",
      "?count=25&after=t3_7ybf3y\n",
      "2\n",
      "?count=25&after=t3_7y92z6\n",
      "3\n",
      "?count=25&after=t3_7yal50\n",
      "4\n",
      "?count=25&after=t3_7ybwrm\n",
      "5\n",
      "?count=25&after=t3_7y8q5l\n",
      "6\n",
      "?count=25&after=t3_7ycceb\n",
      "7\n",
      "?count=25&after=t3_7y8whs\n",
      "8\n",
      "?count=25&after=t3_7y7710\n",
      "9\n",
      "?count=25&after=t3_7y6t3l\n",
      "10\n",
      "?count=25&after=t3_7y71yk\n",
      "11\n",
      "?count=25&after=t3_7yb9af\n",
      "12\n",
      "?count=25&after=t3_7y85at\n",
      "13\n",
      "?count=25&after=t3_7y7zas\n",
      "14\n",
      "?count=25&after=t3_7ybpux\n",
      "15\n",
      "?count=25&after=t3_7y6ypr\n",
      "16\n",
      "?count=25&after=t3_7y8dj3\n",
      "17\n",
      "?count=25&after=t3_7y9je8\n",
      "18\n",
      "?count=25&after=t3_7yc681\n",
      "19\n",
      "?count=25&after=t3_7y6ni1\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "COLUMN_NAMES = ['titles', 'subreddit', 'time', 'comments']\n",
    "df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "\n",
    "end_of_line=''\n",
    "counter=1\n",
    "while counter<21:\n",
    "    url = \"https://www.reddit.com/r/popular/?geo_filter=US\"+end_of_line\n",
    "    response = requests.get(url, headers = {'User-agent': 'Agent_Smith'})\n",
    "\n",
    "    reddit = response.text\n",
    "    soup = BeautifulSoup(reddit, 'lxml')\n",
    "\n",
    "    titles = soup.find_all('a', {'class':'title'})\n",
    "    subreddit = soup.find_all('a', {'class':'subreddit hover may-blank'})\n",
    "    time = soup.find_all('time', {'class':'live-timestamp'})\n",
    "    comments = soup.find_all('a', {'class':'bylink comments may-blank'})\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "\n",
    "    for x in range(len(titles)):\n",
    "        a.append(titles[x].text)\n",
    "        b.append(subreddit[x].text)\n",
    "        c.append(time[x].text)\n",
    "        d.append(comments[x].text)\n",
    "\n",
    "    result = list(zip(a, b, c, d))\n",
    "    small_df = pd.DataFrame(result, columns=COLUMN_NAMES, index=None)\n",
    "    df=pd.concat([df, small_df], ignore_index=True)\n",
    "    \n",
    "    report = soup.find_all('div', {'class':'reportform'})\n",
    "    last_id=report[24]['class'][1].split('-')[1]\n",
    "    end_of_line='?count=25&after='+last_id\n",
    "    print(end_of_line)\n",
    "    print(counter)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/austinlasseter/DSI-EC-2/projects/project-3/February_18-06-51.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "def _getToday():\n",
    "    return datetime.datetime.now().strftime(\"%d-%H-%M\")\n",
    "outpath = os.getcwd()\n",
    "filename = \"%s_%s.%s\" % (\"February\", _getToday() ,\"csv\")\n",
    "df.to_csv(outpath + \"/data/\" + filename, index=False)\n",
    "print (outpath + \"/data/\"  + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
